{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "#!pip install googletrans\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk & Advice ( Travel_Advisory_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful API request\n",
      "Data is extraceted and writtern to a CSV file successfully\n"
     ]
    }
   ],
   "source": [
    "# This is a sample for one country\n",
    "\"\"\"\n",
    "url='https://www.reisewarnung.net/api?country=DE'\n",
    "r = requests.get(url)\n",
    "assert r.status_code==200,'API request failed'\n",
    "print('Successful API request')\n",
    "d=r.json()\n",
    "print(json.dumps(d,indent=4))\n",
    "\"\"\"\n",
    "\n",
    "#Saving advisory of all in one file\n",
    "\n",
    "url='https://www.reisewarnung.net/api'\n",
    "r = requests.get(url)\n",
    "assert r.status_code==200,'API request failed'\n",
    "print('Successful API request')\n",
    "d=r.json()['data']\n",
    "\n",
    "info=[]\n",
    "for i in d.keys():\n",
    "    ccode=i\n",
    "    cname=list(d[i]['lang']['de'].values())[0]\n",
    "    crating=d[i]['situation']['rating']\n",
    "    cadvice=d[i]['lang']['en']['advice']\n",
    "    info.append([ccode,cname,crating,cadvice])\n",
    "df=pd.DataFrame(info,columns=['Code','Country','Risk_Rating','Advice'])\n",
    "df.to_csv('../data/info/advisory_info_from_api.csv',index=False)\n",
    "print('Data is extraceted and writtern to a CSV file successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advisory info from countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import folium\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import (edit_distance,jaccard_distance)\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#!pip install googletrans\n",
    "#!pip install datapackage\n",
    "#! pip install pyspellchecker\n",
    "#!pip install pyenchant\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting geojson data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of countries :  245\n"
     ]
    }
   ],
   "source": [
    "with open('../json_csv_files/custom_world.geo.json') as f:\n",
    "    d0=json.load(f)\n",
    "    \n",
    "\n",
    "with open('../json_csv_files/custom_world.geo.json') as f:\n",
    "    d0=json.load(f)\n",
    "    \n",
    "print('No of countries : ',len(d0['features']))\n",
    "\n",
    "clist0=[ i['properties']['name'] for i in d0['features'] ]\n",
    "\n",
    "#print(sorted(clist0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting filtered country name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../json_csv_files/modified_america_country_names.csv')\n",
    "new_america_countries = list(df.iloc[:,0])\n",
    "\n",
    "df=pd.read_csv('../json_csv_files/modified_austria_country_names.csv')\n",
    "new_austria_countries = list(df.iloc[:,0])\n",
    "\n",
    "df=pd.read_csv('../json_csv_files/modified_canada_country_names.csv')\n",
    "new_canada_countries = list(df.iloc[:,0])\n",
    "\n",
    "df=pd.read_csv('../json_csv_files/modified_australia_country_names.csv')\n",
    "new_australia_countries = list(df.iloc[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### America data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status :  200\n",
      "No of advisories available :  207\n",
      "https://travel.state.gov/content/travel/en/traveladvisories/traveladvisories/Liechtenstein-Travel-Advisory.html\n",
      "***************************************************************************************************************************************************************************************************************Errorlist :  []\n",
      "\n",
      "Extraction for all countries is done from american website\n"
     ]
    }
   ],
   "source": [
    "url = \"https://travel.state.gov/content/travel/en/traveladvisories/traveladvisories.html/\"\n",
    "base_site ='https://travel.state.gov'\n",
    "\n",
    "r=requests.get(url)\n",
    "print('Status : ',r.status_code)\n",
    "\n",
    "html_content = r.content\n",
    "soup=BeautifulSoup(html_content,'lxml')\n",
    "\n",
    "\n",
    "with open('../data/usadvisory.html','wb') as f:\n",
    "    f.write(soup.prettify('utf-8'))\n",
    "    \n",
    "\n",
    "\n",
    "all_links=soup.findAll('a', title=re.compile('Travel Advisory'),href=re.compile('(?i)travel-advisory'))\n",
    "print('No of advisories available : ',len(all_links))\n",
    "\n",
    "all_adv_links = [ urljoin(base_site, i.get('href')) for i in all_links]\n",
    "all_countries = [ ' '.join(i.text.split( )[:-2]) for i in all_links]\n",
    "print(all_adv_links[0])\n",
    "\n",
    "\n",
    "#!mkdir -p ../data/Alldata\n",
    "\n",
    "dir = \"../data/Alldata\"\n",
    "\n",
    "\n",
    "if os.path.exists(dir):\n",
    "    shutil.rmtree(dir)\n",
    "os.makedirs(dir)\n",
    "\n",
    "errlist1=[]\n",
    "\n",
    "for i in range(len(new_america_countries)):\n",
    "    url1=all_adv_links[i]\n",
    "    r=requests.get(url1)\n",
    "    #print('S:',r.status_code,end=' * ')\n",
    "    print('',end='*')\n",
    "\n",
    "    html_content = r.content\n",
    "    soup1=BeautifulSoup(html_content,'lxml')\n",
    "    \n",
    "    text=soup1.find('div',{'class':'tsg-rwd-emergency-alert-text'}).text.replace('\\n',' ').replace('\\xa0','')\n",
    "    text=str(text.encode(sys.stdout.encoding, errors='replace') )\n",
    "    try:\n",
    "        with open('../data/Alldata/'+new_america_countries[i]+'.txt','w') as f:\n",
    "            f.write(text)\n",
    "            f.write('\\n'+'*'*100+'\\n')\n",
    "    except:\n",
    "        #print('Error at index : ',i)\n",
    "        errlist1.append(i)\n",
    "        \n",
    "print('Errorlist : ',errlist1)        \n",
    "print('\\nExtraction for all countries is done from american website')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status :  200\n",
      "Status :  200\n",
      "No of advisories available :  230\n",
      "https://travel.gc.ca/destinations/afghanistan\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@Errorlist :  []\n",
      "\n",
      "Extraction for all countries is done from canada website\n"
     ]
    }
   ],
   "source": [
    "url = \"https://travel.state.gov/content/travel/en/traveladvisories/traveladvisories.html/\"\n",
    "base_site ='https://travel.state.gov'\n",
    "\n",
    "r=requests.get(url)\n",
    "print('Status : ',r.status_code)\n",
    "\n",
    "\n",
    "url = \"https://travel.gc.ca/travelling/advisories\"\n",
    "base_site ='https://travel.gc.ca/'\n",
    "\n",
    "r=requests.get(url)\n",
    "print('Status : ',r.status_code)\n",
    "\n",
    "html_content = r.content\n",
    "soup=BeautifulSoup(html_content,'lxml')\n",
    "\n",
    "\n",
    "with open('../data/canadaadvisory.html','wb') as f:\n",
    "    f.write(soup.prettify('utf-8'))\n",
    "    \n",
    "\n",
    "all_links=soup.findAll('a',href=re.compile('destinations'))\n",
    "print('No of advisories available : ',len(all_links))\n",
    "    \n",
    "    \n",
    "all_adv_links = [ urljoin(base_site, i.get('href')) for i in all_links]\n",
    "canada_countries = [ i.text for i in all_links]\n",
    "print(all_adv_links[0])\n",
    "\n",
    "\n",
    "\n",
    "errlist2=[]\n",
    "\n",
    "for i in range(len(new_canada_countries)):\n",
    "    url1=all_adv_links[i]\n",
    "    try:\n",
    "        r=requests.get(url1)\n",
    "        print('',end='@')\n",
    "\n",
    "        html_content = r.content\n",
    "        soup1=BeautifulSoup(html_content,'lxml')\n",
    "    \n",
    "        text=soup1.find('details',{'id':'risk'}).text.replace('\\n',' ')\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    except:\n",
    "        print('Error at request : ',i)\n",
    "    #text=str(text.encode(sys.stdout.encoding, errors='replace') )\n",
    "    try:\n",
    "        with open('../data/Alldata/'+new_canada_countries[i]+'.txt','a') as f:\n",
    "            f.write(text)\n",
    "            f.write('\\n'+'*'*100+'\\n')\n",
    "    except:\n",
    "        #print('Error at index : ',i)\n",
    "        errlist2.append(i)\n",
    "        \n",
    "print('Errorlist : ',errlist1)         \n",
    "print('\\nExtraction for all countries is done from canada website')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Austria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status :  200\n",
      "No of advisories available :  207\n",
      "https://www.bmeia.gv.at/reise-aufenthalt/reiseinformation/land/afghanistan/\n",
      "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&Errorlist 3-1 :  [56]\n",
      "Errorlist 3-2 :  []\n",
      "Extraction for all countries is done from austria website\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.bmeia.gv.at/reise-aufenthalt/reisewarnungen/\"\n",
    "base_site ='https://www.bmeia.gv.at/'\n",
    "\n",
    "r=requests.get(url)\n",
    "print('Status : ',r.status_code)\n",
    "\n",
    "\n",
    "html_content = r.content\n",
    "soup=BeautifulSoup(html_content,'lxml')\n",
    "\n",
    "\n",
    "with open('../data/germanadvisory.html','wb') as f:\n",
    "    f.write(soup.prettify('utf-8'))\n",
    "    \n",
    "\n",
    "all_links=soup.findAll('a',href=re.compile('reise-aufenthalt/reiseinformation/land'))\n",
    "all_links=all_links[:207]  \n",
    "print('No of advisories available : ',len(all_links))\n",
    "\n",
    "all_adv_links = [ urljoin(base_site, i.get('href')) for i in all_links]\n",
    "all_countries = [ i.text.strip() for i in all_links]\n",
    "print(all_adv_links[0])\n",
    "\n",
    "\n",
    "translator = Translator()\n",
    "errlist3_1=[]\n",
    "errlist3_2=[]\n",
    "\n",
    "for i in range(0,len(all_countries)):\n",
    "    url1=all_adv_links[i]\n",
    "    try:\n",
    "        r=requests.get(url1)\n",
    "        print('',end='&')\n",
    "\n",
    "        html_content = r.content\n",
    "        soup1=BeautifulSoup(html_content,'lxml')\n",
    "    \n",
    "        ger_text = soup1.find('div',{'class':'panel-body'}).text\n",
    "        eng_text = translator.translate(ger_text, src='de', dest='en')\n",
    "        text     = eng_text.text.encode('ascii', 'ignore').decode('ascii')\n",
    "    except:\n",
    "        #print('Error at request/encoding :',i)\n",
    "        errlist3_1.append(i)\n",
    "    \n",
    "    try:\n",
    "        with open('../data/Alldata/'+new_austria_countries[i]+'.txt','a') as f:\n",
    "            f.write(text)\n",
    "            f.write('\\n'+'+'*100+'\\n')\n",
    "    except:\n",
    "        print('Error at index : ',i)\n",
    "        errlist3_2.append(i)\n",
    "        \n",
    "print('Errorlist 3-1 : ',errlist3_1)         \n",
    "print('Errorlist 3-2 : ',errlist3_2)         \n",
    "print('Extraction for all countries is done from austria website')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
